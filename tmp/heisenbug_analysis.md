# ğŸ” ä¸ºä»€ä¹ˆæ·»åŠ è°ƒè¯•ä»£ç åå°±èƒ½ç”Ÿæˆ PPT äº†ï¼Ÿ

è¿™æ˜¯ä¸€ä¸ªéå¸¸æœ‰è¶£çš„ç°è±¡ï¼è®©æˆ‘æ·±å…¥åˆ†æåŸå› ï¼š

---

## ğŸ¯ æ ¸å¿ƒåŸå› ï¼š**å¼‚æ­¥ç”Ÿæˆå™¨çš„å»¶è¿Ÿæ‰§è¡Œé—®é¢˜**

### é—®é¢˜æœ¬è´¨

Python çš„**å¼‚æ­¥ç”Ÿæˆå™¨ï¼ˆAsyncGeneratorï¼‰**æœ‰ä¸€ä¸ªç‰¹æ€§ï¼š**å®ƒæ˜¯æƒ°æ€§æ‰§è¡Œçš„**ã€‚

```python
# è¿™ä¸ªå‡½æ•°ä¸ä¼šç«‹å³æ‰§è¡Œ
async def _stream_openai(...) -> AsyncGenerator[str, None]:
    async for event in await client.chat.completions.create(...):
        content_chunk = event.choices[0].delta.content
        if content_chunk:
            yield content_chunk  # â† åªæœ‰åœ¨è¢«æ¶ˆè´¹æ—¶æ‰ä¼šæ‰§è¡Œ
```

**å…³é”®ç‚¹**ï¼š
- `yield` è¯­å¥åªæœ‰åœ¨**æœ‰äººæ¶ˆè´¹ï¼ˆè¿­ä»£ï¼‰è¿™ä¸ªç”Ÿæˆå™¨**æ—¶æ‰ä¼šæ‰§è¡Œ
- å¦‚æœè°ƒç”¨æ–¹æ²¡æœ‰æ­£ç¡®æ¶ˆè´¹ï¼Œç”Ÿæˆå™¨ä¼š"ä¼‘çœ "
- æ·»åŠ  `print()` è¯­å¥åï¼Œ**å¼ºåˆ¶ Python æ‰§è¡Œåˆ°é‚£ä¸€è¡Œ**ï¼Œé—´æ¥æ¨åŠ¨äº†ç”Ÿæˆå™¨çš„è¿è¡Œ

---

## ğŸ› ä¹‹å‰ä¸ºä»€ä¹ˆä¼šå¡ä½ï¼Ÿ

### åœºæ™¯ 1ï¼šå‰ç«¯æ²¡æœ‰æ­£ç¡®æ¶ˆè´¹æµå¼æ•°æ®

```typescript
// âŒ é”™è¯¯çš„å‰ç«¯ä»£ç ï¼ˆå¯èƒ½çš„æƒ…å†µï¼‰
const response = await fetch('/api/generate');
const data = await response.json();  // â† æœŸæœ›ä¸€æ¬¡æ€§è·å–å®Œæ•´æ•°æ®
// ä½†åç«¯è¿”å›çš„æ˜¯æµå¼æ•°æ®ï¼ˆstreamï¼‰ï¼Œéœ€è¦é€å—è¯»å–
```

**é—®é¢˜**ï¼š
- åç«¯å·²ç»å‡†å¤‡å¥½æµå¼è¾“å‡º
- ä½†å‰ç«¯åœ¨ç­‰å¾…**å®Œæ•´çš„ JSON å“åº”**
- æµå¼æ•°æ®æ²¡æœ‰è¢«æ¶ˆè´¹ â†’ ç”Ÿæˆå™¨ä¼‘çœ  â†’ çœ‹èµ·æ¥"å¡ä½"äº†

---

### åœºæ™¯ 2ï¼šåç«¯æµå¼å“åº”æ²¡æœ‰è¢«æ­£ç¡® flush

```python
# âŒ ä¹‹å‰çš„ä»£ç ï¼ˆç®€åŒ–ç‰ˆï¼‰
async def _stream_openai(...):
    async for event in await client.chat.completions.create(...):
        content_chunk = event.choices[0].delta.content
        if content_chunk:
            yield content_chunk  # â† æ•°æ®å¯èƒ½è¢«ç¼“å†²ï¼Œæ²¡æœ‰ç«‹å³å‘é€
```

**é—®é¢˜**ï¼š
- Python çš„è¾“å‡ºé»˜è®¤æœ‰**ç¼“å†²åŒº**
- æ•°æ®ä¼šç´¯ç§¯åˆ°ä¸€å®šå¤§å°æ‰å‘é€
- å‰ç«¯åœ¨ç­‰å¾…ç¬¬ä¸€ä¸ªæ•°æ®å— â†’ è¶…æ—¶æˆ–å¡ä½

**æ·»åŠ è°ƒè¯•ä»£ç å**ï¼š
```python
# âœ… ç°åœ¨çš„ä»£ç 
async def _stream_openai(...):
    print("\nğŸ›‘ [DEBUG] å¼€å§‹æ¥æ”¶æµå¼æ•°æ®: ", end="", flush=True)
    #                                                    â†‘â†‘â†‘â†‘â†‘
    #                                           flush=True å¼ºåˆ¶åˆ·æ–°ç¼“å†²åŒºï¼
    
    async for event in await client.chat.completions.create(...):
        content_chunk = event.choices[0].delta.content
        if content_chunk:
            print(content_chunk, end="", flush=True)  # â† æ¯æ¬¡éƒ½åˆ·æ–°
            yield content_chunk
```

**`flush=True` çš„ä½œç”¨**ï¼š
- ç«‹å³å°†æ•°æ®ä»ç¼“å†²åŒºå‘é€åˆ°ç»ˆç«¯ï¼ˆæˆ–å“åº”æµï¼‰
- ä¸ç­‰å¾…ç¼“å†²åŒºæ»¡
- **é—´æ¥è§¦å‘äº†ç½‘ç»œæ•°æ®çš„å³æ—¶å‘é€**

---

### åœºæ™¯ 3ï¼šå¼‚æ­¥äº‹ä»¶å¾ªç¯çš„è°ƒåº¦é—®é¢˜

```python
# âŒ ä¹‹å‰çš„ä»£ç 
async def _stream_openai(...):
    async for event in ...:
        if content_chunk:
            yield content_chunk  # â† åªæ˜¯ yieldï¼Œæ²¡æœ‰å…¶ä»–æ“ä½œ
```

**é—®é¢˜**ï¼š
- å¦‚æœç”Ÿæˆå™¨å†…éƒ¨**æ²¡æœ‰ä»»ä½• I/O æ“ä½œ**ï¼ˆå¦‚ printï¼‰ï¼ŒPython çš„äº‹ä»¶å¾ªç¯å¯èƒ½ä¸ä¼šåŠæ—¶è°ƒåº¦
- å¯¼è‡´æ•°æ®ç§¯å‹ï¼Œçœ‹èµ·æ¥åƒå¡ä½

**æ·»åŠ  `print()` å**ï¼š
```python
# âœ… ç°åœ¨çš„ä»£ç 
async def _stream_openai(...):
    async for event in ...:
        if content_chunk:
            print(content_chunk, end="", flush=True)  # â† è¿™æ˜¯ä¸€ä¸ª I/O æ“ä½œï¼
            yield content_chunk
```

**`print()` çš„å‰¯ä½œç”¨**ï¼š
- è§¦å‘äº†**ç³»ç»Ÿè°ƒç”¨ï¼ˆsys.stdout.writeï¼‰**
- ç»™äº‹ä»¶å¾ªç¯ä¸€ä¸ª"è°ƒåº¦ç‚¹"
- è®© Python æœ‰æœºä¼šå¤„ç†å…¶ä»–å¾…å¤„ç†çš„ä»»åŠ¡ï¼ˆå¦‚å‘é€ç½‘ç»œæ•°æ®ï¼‰

---

## ğŸ”¬ æŠ€æœ¯æ·±å…¥ï¼šä¸ºä»€ä¹ˆ print() ä¼šå½±å“å¼‚æ­¥æµï¼Ÿ

### Python å¼‚æ­¥ç”Ÿæˆå™¨çš„æ‰§è¡Œæ¨¡å‹

```python
async def generator():
    for i in range(10):
        yield i  # â† è¿™é‡Œä¼šæš‚åœï¼Œç­‰å¾…æ¶ˆè´¹è€…

# æ¶ˆè´¹è€…
async for value in generator():
    print(value)  # â† è¿™é‡Œè§¦å‘ä¸‹ä¸€æ¬¡è¿­ä»£
```

**å…³é”®**ï¼š
- `yield` åï¼Œç”Ÿæˆå™¨è¿›å…¥"æŒ‚èµ·"çŠ¶æ€
- åªæœ‰æ¶ˆè´¹è€…è°ƒç”¨ `__anext__()` æ—¶æ‰ä¼šæ¢å¤
- å¦‚æœæ¶ˆè´¹è€…æ²¡æœ‰åŠæ—¶è°ƒç”¨ â†’ **å¡ä½**

---

### FastAPI çš„æµå¼å“åº”æœºåˆ¶

```python
# FastAPI ä¸­çš„æµå¼å“åº”
from fastapi.responses import StreamingResponse

@app.get("/stream")
async def stream_endpoint():
    async def event_generator():
        async for chunk in llm_client.stream(...):
            yield f"data: {chunk}\n\n"  # â† SSE æ ¼å¼
    
    return StreamingResponse(event_generator(), media_type="text/event-stream")
```

**å¯èƒ½çš„é—®é¢˜**ï¼š
- å¦‚æœ `llm_client.stream(...)` æ²¡æœ‰ç«‹å³äº§ç”Ÿæ•°æ®
- FastAPI ä¼šç­‰å¾…ç¬¬ä¸€ä¸ª `yield`
- å‰ç«¯ä¹Ÿåœ¨ç­‰å¾…ç¬¬ä¸€ä¸ªæ•°æ®å—
- å¦‚æœç”Ÿæˆå™¨å†…éƒ¨æ²¡æœ‰è§¦å‘æ‰§è¡Œ â†’ **åŒå‘ç­‰å¾…ï¼Œæ­»é”**

**æ·»åŠ  `print()` å**ï¼š
- `print()` å¼ºåˆ¶ç”Ÿæˆå™¨æ‰§è¡Œåˆ° `yield` è¯­å¥
- æ•°æ®ç«‹å³è¢« `yield` å‡ºå»
- FastAPI ç«‹å³å‘é€ç»™å‰ç«¯
- æ­»é”è¢«æ‰“ç ´ï¼

---

## ğŸ“Š å¯¹æ¯”ï¼šæ·»åŠ è°ƒè¯•ä»£ç å‰å

### ä¹‹å‰çš„æ‰§è¡Œæµç¨‹ï¼ˆå¡ä½ï¼‰

```
1. å‰ç«¯å‘èµ·è¯·æ±‚ â†’ åç«¯å¼€å§‹ç”Ÿæˆ
   â†“
2. åç«¯è°ƒç”¨ llm_client.stream(...)
   â†“
3. ç”Ÿæˆå™¨åˆ›å»ºï¼Œä½†æ²¡æœ‰è¢«ç«‹å³æ‰§è¡Œï¼ˆæƒ°æ€§ï¼‰
   â†“
4. FastAPI ç­‰å¾…ç¬¬ä¸€ä¸ª yield æ•°æ®
   â†“
5. ç”Ÿæˆå™¨å†…éƒ¨æ²¡æœ‰å¼ºåˆ¶æ‰§è¡Œçš„ä»£ç ï¼ˆæ²¡æœ‰ printï¼‰
   â†“
6. æ•°æ®åœ¨ Qwen API å’Œ Python ä¹‹é—´çš„æŸä¸ªç¼“å†²åŒºç´¯ç§¯
   â†“
7. å‰ç«¯è¶…æ—¶æˆ–ä¸€ç›´ç­‰å¾…ï¼ˆçœ‹èµ·æ¥å¡ä½ï¼‰
   â†“
8. ç”¨æˆ·åˆ·æ–°é¡µé¢ï¼Œä¹‹å‰çš„è¯·æ±‚è¢«å–æ¶ˆ
```

---

### ç°åœ¨çš„æ‰§è¡Œæµç¨‹ï¼ˆæ­£å¸¸ï¼‰

```
1. å‰ç«¯å‘èµ·è¯·æ±‚ â†’ åç«¯å¼€å§‹ç”Ÿæˆ
   â†“
2. åç«¯è°ƒç”¨ llm_client.stream(...)
   â†“
3. ç”Ÿæˆå™¨ç«‹å³æ‰§è¡Œåˆ°ç¬¬ä¸€ä¸ª print()
   â†“  print("\nğŸ›‘ [DEBUG] å¼€å§‹æ¥æ”¶æµå¼æ•°æ®: ", end="", flush=True)
   â†“
4. flush=True å¼ºåˆ¶åˆ·æ–°æ ‡å‡†è¾“å‡ºç¼“å†²åŒº
   â†“  ï¼ˆå‰¯ä½œç”¨ï¼šè§¦å‘äº‹ä»¶å¾ªç¯è°ƒåº¦ï¼‰
   â†“
5. è¿›å…¥ async for å¾ªç¯ï¼Œå¼€å§‹æ¥æ”¶ Qwen çš„æ•°æ®
   â†“
6. æ¯æ”¶åˆ°ä¸€ä¸ª chunkï¼Œç«‹å³ print()
   â†“  print(content_chunk, end="", flush=True)
   â†“
7. ç«‹å³ yield ç»™ FastAPI
   â†“  yield content_chunk
   â†“
8. FastAPI ç«‹å³å‘é€ç»™å‰ç«¯ï¼ˆSSE æ ¼å¼ï¼‰
   â†“  data: chunk1\n\n
   â†“  data: chunk2\n\n
   â†“
9. å‰ç«¯é€æ­¥æ¥æ”¶æ•°æ®ï¼Œå®æ—¶æ˜¾ç¤º
   â†“
10. ç”Ÿæˆå®Œæˆï¼Œå‰ç«¯è·³è½¬åˆ°é¢„è§ˆé¡µé¢ âœ…
```

---

## ğŸ¯ çœŸæ­£çš„ä¿®å¤æ–¹æ³•ï¼ˆä¸ä¾èµ– printï¼‰

è™½ç„¶ `print()` è§£å†³äº†é—®é¢˜ï¼Œä½†è¿™ä¸æ˜¯æ­£ç¡®çš„ä¿®å¤æ–¹æ³•ã€‚çœŸæ­£çš„ä¿®å¤åº”è¯¥æ˜¯ï¼š

### æ–¹æ³• 1ï¼šç¡®ä¿æµå¼å“åº”è¢«æ­£ç¡®æ¶ˆè´¹

```python
# åœ¨ FastAPI è·¯ç”±ä¸­
@app.post("/api/v1/ppt/presentation/generate")
async def generate_presentation(...):
    async def event_stream():
        async for chunk in llm_client.stream(...):
            # ç«‹å³å‘é€ï¼Œä¸ç­‰å¾…ç¼“å†²
            yield f"data: {json.dumps({'chunk': chunk})}\n\n"
            
            # å…³é”®ï¼šä¸»åŠ¨è®©å‡ºæ§åˆ¶æƒç»™äº‹ä»¶å¾ªç¯
            await asyncio.sleep(0)  # â† è¿™è¡Œå¾ˆé‡è¦ï¼
    
    return StreamingResponse(
        event_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "X-Accel-Buffering": "no",  # ç¦ç”¨ Nginx ç¼“å†²ï¼ˆå¦‚æœæœ‰ï¼‰
        }
    )
```

---

### æ–¹æ³• 2ï¼šåœ¨ç”Ÿæˆå™¨å†…éƒ¨ä¸»åŠ¨åˆ·æ–°

```python
async def _stream_openai(...):
    async for event in await client.chat.completions.create(...):
        content_chunk = event.choices[0].delta.content
        if content_chunk:
            yield content_chunk
            
            # ä¸»åŠ¨è®©å‡ºæ§åˆ¶æƒï¼Œè®©äº‹ä»¶å¾ªç¯å¤„ç†å…¶ä»–ä»»åŠ¡
            await asyncio.sleep(0)  # â† å…³é”®ï¼
```

**`await asyncio.sleep(0)` çš„ä½œç”¨**ï¼š
- å‘Šè¯‰äº‹ä»¶å¾ªç¯ï¼š"æˆ‘å¯ä»¥æš‚åœä¸€ä¸‹ï¼Œä½ å»å¤„ç†å…¶ä»–ä»»åŠ¡"
- è§¦å‘ç½‘ç»œæ•°æ®çš„å‘é€
- é¿å…æ•°æ®ç§¯å‹åœ¨ç¼“å†²åŒº

---

### æ–¹æ³• 3ï¼šé…ç½® Uvicorn ç¦ç”¨ç¼“å†²

```bash
# å¯åŠ¨å‘½ä»¤
uvicorn api.main:app \
    --reload \
    --host 0.0.0.0 \
    --port 8000 \
    --timeout-keep-alive 300 \
    --limit-concurrency 1000 \
    --backlog 2048 \
    --ws-ping-interval 20 \
    --ws-ping-timeout 20
```

æˆ–åœ¨ä»£ç ä¸­ï¼š

```python
import uvicorn

if __name__ == "__main__":
    uvicorn.run(
        "api.main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        timeout_keep_alive=300,  # ä¿æŒè¿æ¥ 5 åˆ†é’Ÿ
        limit_concurrency=1000,
        access_log=True,
    )
```

---

## ğŸ‰ æ€»ç»“

### ä¸ºä»€ä¹ˆæ·»åŠ  `print()` åå°±èƒ½å·¥ä½œäº†ï¼Ÿ

```
1. flush=True å¼ºåˆ¶åˆ·æ–°è¾“å‡ºç¼“å†²åŒº
   â†’ è§¦å‘ç³»ç»Ÿ I/O è°ƒç”¨
   â†’ ç»™äº‹ä»¶å¾ªç¯ä¸€ä¸ªè°ƒåº¦æœºä¼š
   â†’ ç½‘ç»œæ•°æ®è¢«åŠæ—¶å‘é€

2. print() æ˜¯ä¸€ä¸ª I/O æ“ä½œ
   â†’ æ‰“æ–­äº†çº¯è®¡ç®—é€»è¾‘
   â†’ è®© Python æœ‰æœºä¼šå¤„ç†å…¶ä»–å¾…å¤„ç†ä»»åŠ¡
   â†’ æµå¼æ•°æ®ä¸å†ç§¯å‹

3. è°ƒè¯•ä»£ç çš„æ‰§è¡Œæœ¬èº«
   â†’ æ”¹å˜äº†ä»£ç çš„æ‰§è¡Œæ—¶åº
   â†’ åŸæœ¬çš„ç«æ€æ¡ä»¶è¢«æ¶ˆé™¤
   â†’ é—®é¢˜"æ„å¤–"è§£å†³
```

---

### è¿™æ˜¯ä¸€ä¸ªç»å…¸çš„"Heisenbug"

**Heisenbug**ï¼šä¸€ç§åœ¨å°è¯•ç ”ç©¶æˆ–è°ƒè¯•æ—¶ä¼šæ¶ˆå¤±æˆ–æ”¹å˜çš„ bugã€‚

- æ·»åŠ è°ƒè¯•ä»£ç  â†’ bug æ¶ˆå¤±
- ç§»é™¤è°ƒè¯•ä»£ç  â†’ bug é‡ç°
- **æ ¹æœ¬åŸå› **ï¼šè°ƒè¯•ä»£ç æ”¹å˜äº†ç¨‹åºçš„æ‰§è¡Œæ—¶åº

---

### æ­£ç¡®çš„ä¿®å¤æ–¹æ¡ˆ

```python
# âœ… æ¨èçš„ä¿®å¤ï¼ˆä¸ä¾èµ– printï¼‰
async def _stream_openai(...):
    async for event in await client.chat.completions.create(...):
        content_chunk = event.choices[0].delta.content
        if content_chunk:
            yield content_chunk
            await asyncio.sleep(0)  # ä¸»åŠ¨è®©å‡ºæ§åˆ¶æƒ
```

æˆ–è€…åœ¨ FastAPI è·¯ç”±ä¸­ï¼š

```python
async def event_stream():
    async for chunk in llm_client.stream(...):
        yield f"data: {chunk}\n\n"
        await asyncio.sleep(0)  # ç¡®ä¿æ•°æ®åŠæ—¶å‘é€
```

---

### å»ºè®®

1. **ä¿ç•™è°ƒè¯•ä»£ç **ï¼ˆçŸ­æœŸï¼‰ï¼šç°åœ¨èƒ½ç”¨å°±å…ˆåˆ«åŠ¨
2. **æ·»åŠ  `asyncio.sleep(0)`**ï¼ˆé•¿æœŸï¼‰ï¼šè¿™æ˜¯æ›´è§„èŒƒçš„ä¿®å¤
3. **æ£€æŸ¥å‰ç«¯ä»£ç **ï¼šç¡®ä¿æ­£ç¡®æ¶ˆè´¹ SSE æµ
4. **é…ç½®æœåŠ¡å™¨**ï¼šç¦ç”¨ä¸å¿…è¦çš„ç¼“å†²

---

## ğŸ“ ç›¸å…³ä»£ç ä½ç½®

### å·²ä¿®æ”¹çš„æ–‡ä»¶

1. **`servers/fastapi/services/llm_client.py`** - æ·»åŠ äº†è°ƒè¯•è¾“å‡º
   - ç¬¬ 208-213 è¡Œï¼šéæµå¼ç”Ÿæˆè°ƒè¯•
   - ç¬¬ 404 è¡Œï¼šCustom LLM è°ƒç”¨è¿½è¸ª
   - ç¬¬ 860ã€875-878 è¡Œï¼šæµå¼ç”Ÿæˆå®æ—¶è¾“å‡º
   - ç¬¬ 1203ã€1237ã€1270 è¡Œï¼šç»“æ„åŒ–æµå¼ç”Ÿæˆè°ƒè¯•

### å…·ä½“ä¿®æ”¹å†…å®¹

```python
# ä¿®æ”¹ 1: _generate_openai (éæµå¼)
try:
    debug_content = response.choices[0].message.content
    print(f"\nğŸ›‘ [DEBUG] Qwen (Non-Stream) è¿”å›å†…å®¹:\n{debug_content}\nğŸ›‘ [DEBUG End]\n")
except Exception as e:
    print(f"ğŸ›‘ [DEBUG] æ‰“å°å‡ºé”™: {e}")

# ä¿®æ”¹ 2: _generate_custom
print(f"\nğŸ›‘ [DEBUG] Custom LLM (_generate_custom) è¢«è°ƒç”¨ï¼Œå‡†å¤‡ä½¿ç”¨ OpenAI å…¼å®¹æ¥å£\n")

# ä¿®æ”¹ 3: _stream_openai (æµå¼)
print("\nğŸ›‘ [DEBUG] å¼€å§‹æ¥æ”¶æµå¼æ•°æ®: ", end="", flush=True)
# å¾ªç¯å†…
if content_chunk:
    print(content_chunk, end="", flush=True)

# ä¿®æ”¹ 4: _stream_openai_structured (ç»“æ„åŒ–æµå¼)
print("\nğŸ›‘ [DEBUG Structured] å¼€å§‹æ¥æ”¶ç»“æ„åŒ–æµå¼æ•°æ®: ", end="", flush=True)
# ä¸¤å¤„ yield å‰
print(content_chunk, end="", flush=True)
print(tool_arguments, end="", flush=True)
```

---

## ğŸ” ä¸‹ä¸€æ­¥ä¼˜åŒ–å»ºè®®

### 1. è§„èŒƒåŒ–ä¿®å¤ï¼ˆæ¨èï¼‰

åœ¨æ‰€æœ‰ `yield` è¯­å¥åæ·»åŠ  `await asyncio.sleep(0)`ï¼š

```python
if content_chunk:
    yield content_chunk
    await asyncio.sleep(0)  # è®©å‡ºæ§åˆ¶æƒ
```

### 2. å‰ç«¯ä¼˜åŒ–

ç¡®ä¿å‰ç«¯æ­£ç¡®å¤„ç† SSE æµï¼š

```typescript
const eventSource = new EventSource('/api/generate');
eventSource.onmessage = (event) => {
    const data = JSON.parse(event.data);
    // å¤„ç†æ¯ä¸ªæ•°æ®å—
};
```

### 3. ç›‘æ§å’Œæ—¥å¿—

æ·»åŠ ç»“æ„åŒ–æ—¥å¿—è®°å½•ï¼š

```python
import logging

logger = logging.getLogger(__name__)

async def _stream_openai(...):
    logger.info("Starting stream generation", extra={
        "model": model,
        "max_tokens": max_tokens
    })
    
    chunk_count = 0
    async for event in ...:
        if content_chunk:
            chunk_count += 1
            yield content_chunk
            await asyncio.sleep(0)
    
    logger.info(f"Stream completed", extra={"chunk_count": chunk_count})
```

---

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ"åŠ ä¸ª print å°±å¥½äº†"çš„ç¥å¥‡åŸå› ï¼è¿™åœ¨å¼‚æ­¥ç¼–ç¨‹ä¸­æ˜¯ä¸€ä¸ªå¸¸è§çš„é™·é˜± ğŸ˜Š
